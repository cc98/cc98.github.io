---
layout: post
title: 反作弊策略思想
date: 2012-02-11 20:39:47.000000000 -08:00
type: post
parent_id: '0'
published: true
password: ''
status: publish
categories:
- Share
tags: []
meta:
  _edit_last: '1'
  bot_views: '2'
  views: '6526'
author:
  login: navins
  email: navins@qq.com
  display_name: navins
  first_name: ''
  last_name: ''
permalink: "/share/416.html"
---
从网上看到，觉得归纳的亮点说着挺有道理。

这要从搜索引擎反作弊策略说起。一个搜索引擎成功的反作弊策略一定是这样的：

1，允许算法被探测出来，而且即使算法被公布，搜索结果的公正性都不会受太大影响。要这么做的原因就是不希望和作弊的人陷入到一种猫捉老鼠的死循环当中。如果老是以堵漏洞的做法来修正算法，那永远都没有尽头。出于这样的考虑，搜索引擎会把那些无法被作弊的的因素在排序算法里放到比较重要的程度。

2，尽可能用一切技术手段自动检测，当技术手段不能解决问题，就用人工来解决。然后把人工发现的问题又反馈给自动检测机制，使自动检测越来越完善。

现在的google基本上就是这样来做的。在现有的排序规则中，那些无法作弊的和能精确反应内容的因素，都是很重要的排序因素。

当然Google也不排斥频繁的调整算法，这也有出于给用户一个最好的搜索体验考虑的。

至于技术检测和人工审查，google也一直在做。

<!--more-->

google很早就有匿名蜘蛛来检测一个网站是不是在作弊的。如果去分析网站的服务器LOG日志，就会发现它们。

你会发现，有的爬虫，通过IP查询是来自google，但是它没有自己的声明（user-agent），这就是google的匿名爬虫。它会判断你有没有对google爬虫特别对待，做一些隐藏页面，还会解析Javascript文件和CSS文件等等。有人用CCS文件来隐藏内容，这种事情现在是不用去做的，google都能查出来。

Google也有人工审核机制，从webmaster tool 里提交的问题，都是有人工跟进审核的。以下就是号称google内部流传出来审核规则，可以点此下载。

既然google反作弊那么优秀，那文章一开始提到的那个作弊方法怎么解决呢？

那个方法google确实检测不出来，但是用这个方法的人，到最后还是会被google发现作弊。

google的反作弊是“善意原则”优先，是假设你这个网站是没有作弊的，但是用其他所有作弊的特征来检查。 用了我提到的这个方法，在用颜色隐藏内容这一块是没事了，但是会在堆砌关键词，反向链接，以及其他很多方面路出马脚来。google就是相信，一个在页面上隐藏内容的人，也一定会去做垃圾链接群发等等其他作弊的事情。就像现实生活中一个吸毒的人，当然也是爱打架的，或者爱偷东西的，总有一件事情让你进局子里。

而你假设其他什么都不做，就是用那个方法隐藏一点内容，其实你也不能得到什么。因为你仅仅是隐藏内容的话也不会有排名的。

google就是这样捍卫了自己排名的公正性。

对这些了解得越多，就越发现作弊实在是费力不讨好了。（作弊源于不了解，通过正常途径提升SEO流量的方法有的是，为什么放弃那么多好的方法而选择差的方法呢？在现在的SEO界，你会发现一个现象，越是SEO刚入门的人越喜欢搞一些作弊的事情，而SEO从业越久的人，就越不会参与这些。）

想做一个优秀SEOer的人，对所有这些因素都要有一定程度的了解的。这样做即可以避开无意中犯下的错，又可以避免不必要的恐慌。

比如沙盒效应，很多人总觉得很神秘，其实从搜索引擎的角度出发没什么好神秘的。你要是站在搜索引擎的角度考虑问题，就觉得这是一个很有必要的措施了。你也会知道如何发展自己的外部链接。避免google的反作弊手段落到你网站上。

还有，关于重复内容，google一定是“善意原则”优先的，它甚至会帮你处理掉因为网站大量采用模板带来的重复问题。

要做到了解这些，就是不断的实践，学习和实验。

最近的美剧《Lie to me》非常好看，有一个印象我很深刻，就是他们会定期做一些实验，来了解人类各种复杂微妙的表情后面隐藏着怎样的心理活动，会定期形成报告。这是一种非常好的研究程序。

面对google，我们就像那些心理学家面对人类的心理一样，很多东西是你不了解的。你去测试，就能得到独家的资料和信息。这也是我博客很多东西的来源。

